---
title: "06_tuning"
output: html_document
---

layout: false
class: middle, center, inverse

# 4.5 `dials`:<br><br>Tools for Creating Tuning Parameter Values

---

background-image: url(https://www.tidymodels.org/images/dials.png)
background-position: 97.5% 5%
background-size: 7%
layout: true

---

## 4.5 `dials`: Creating Hyperparameter Values

Most machine learning models require the user to pre-define so-called **hyperparameters** (or *tuning parameters*) prior to model fitting. For example:
- **Linear regression:** -
- **Logistic regression:** -
- **Linear discriminant analysis:** -
- **Regularized regression:** `penalty`, `mixture`
- **NaÃ¯ve bayes:** `Laplace`
- **k-nearest-neighbor:** `neighbors`, `weight_func`, `dist_power` 
- **CART:** `cost_complexity`, `tree_depth`, `min_n`
- **SVM:** `kernel`, `cost`, `degree`, `scale_factor`
- **Bagging:** `trees`, `min_n`
- **Random forest:** `trees`, `mtry`, `min_n`
- **Boosting:** `trees`, `mtry`, `min_n`, `tree_depth`, `learn_rate`

.footnote[
*Note: This list is not exhaustive! Depending on the engine an even broader set of hyperparameters can be specified. Use `args()` to inspect all hyperparameters (i.e. function arguments) available in a `parsnip` object.*
]

???
- hyperparameters cannot be learned from the data (which is why they differ from model coefficients/weights) -> they are external to model training
- NB: Laplace correction for smoothing low-frequency counts.
- CART: cost complexity for pruning as well as max tree depth, min_n for minimum number of data points to allow another split
- svm: scale_factor = gamma -> determines the influence of a single data point on the decision boundary
- boosting: learn_rate -> speed with which the boosted tree adapts to the fitted errors

---

## 4.5 `dials`: Creating Hyperparameter Values

`dials` streamlines the handling of hyperparameters. It provides functions for specifying hyperparameter sequences as well as grids, `parameters` objects that can be processed by the `parsnip` package, and ensures consistent parameter names.

--

In the context of a **regularized regression**, `penalty` and `mixture` are the two central hyperparameters. `dials` comes with a pre-defined `parameters` object for both.
.pull-left[
```{r}
mixture()
```
]
.pull-right[
```{r}
penalty()
```
]

???
- description of the hyperparameter
- indicator if the hyperparameter is quantitative or qualitative
- range of default parameter values
- scale (e.g., linear or logscale)

---

layout: false

## Excursus: Hyperparameters on the Log Scale

**Hyperparameter scaling:** In practice, you will often find that hyperparameters are defined on the log instead of the original linear scale. There are two prominent rationals for preferring a log over a linear scale:

1. Usually, you have no clue regarding the potential optimum. Hence, you are inclined to evaluate a large search space with extremely small but also very large candidate values (e.g., $[0.000001; 10,000]$-interval). On a linear scale, this approach would ignore a large proportion of the search space.

  - **Linear scale:**
  
  `> 0.000001  1000  2000  3000  4000  5000  6000  7000  8000  9000 10000`
  
  - **Log scale:**
  
  `> 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02 1e+03`

2. For some models and hyperparameter sets, the model's accuracy is relatively insensitive to certain regions of the search space. Those models demand a large variation in order to observe any impact on the underlying performance metrics.

If you have identified a promising parameter subspace, you may eventually narrow it down by further restricting the search space of your hyperparameter grid.

---

background-image: url(https://www.tidymodels.org/images/dials.png)
background-position: 97.5% 5%
background-size: 7%
layout: true

---

## 4.5 `dials`: Creating Hyperparameter Values

`parameters` objects are accompanied by various helper functions to query and specify the hyperparameters, e.g., the **parameter ranges**.
.pull-left[
```{r}
penalty() %>% # default
  range_get()
```
]
.pull-right[
```{r}
penalty(range = c(-10, 10)) %>% # custom
  range_get()
```
```{r}
penalty() %>% 
  range_set(c(-10, 10)) # custom
```
]

---

## 4.5 `dials`: Creating Hyperparameter Values

`parameters` objects are accompanied by various helper functions to query and specify the hyperparameters, e.g., the **parameter values**.
```{r}
penalty() %>% 
  value_sample(n = 6) # draw six parameter values with replacement (random line search)
```
```{r}
penalty() %>% 
  value_seq(n = 6, original = F) # draw a sequence of six parameter values (line search)
```
```{r}
penalty() %>% 
  value_set(seq(-10, 0, by = 2)) # set the concrete parameter values
```

.footnote[.pull-right[
*Note: The same helper functions can be applied to qualitative hyperparameters, such as `weight_func()` in `parsnip::nearest_neighbor()`.*
]]

???
- original: if it should return the values on original scale (log) or on the transformed scale

---

## 4.5 `dials`: Creating Hyperparameter Values

There are special cases where the concrete hyperparameter values depend on your data set, e.g., the `mtry` argument (number of randomly sampled predictors at each split) in `parsnip::rand_forest()`.
```{r}
mtry()
```
Therefore, we must `finalize()` the hyperparameter set-up based on the training set.
```{r}
finalize(mtry(), x = train_set %>% select(-died))
```

---

## 4.5 `dials`: Creating Hyperparameter Values

Finally, `dials` renders the systematic querying and evaluation of multiple hyperparameters possible. The simultaneous optimization of multiple hyperparameters is referred to as **(random) grid search**.

--

.pull-left[
```{r}
grid_random(
  mixture(), penalty(),
  size = 25
)
```
]

--

.pull-right[
```{r}
grid_regular(
  mixture(), penalty(range = c(-10, 10)),
  levels = c(5, 5)
)
```
]

???
- there are other, more advanced search procedures, e.g., Tree Parzen or Bayesian Optimization
- generally, grid search is a brute-force like method which is not really efficient (usually it makes more sense to refer to Bayesian techniques while specifying reasonable priors)

---

layout: false
class: middle, center, inverse

# 4.6 `tune`:<br><br>Tidy Tuning Tools

---

background-image: url(https://www.tidymodels.org/images/tune.png)
background-position: 97.5% 5%
background-size: 7%
layout: true

---

## 4.6 `tune`: Tidy Tuning Tools

The `tune` package unites the previous steps in the context of hyperparameter tuning with the **`tune_grid()`** function being the primary modeling workhorse.
```{r, eval=F}
tune_grid(
  object, preprocessor, resamples, #<<
  grid = 10, metrics = NULL, control = control_grid()
)
```
**Positional arguments:**
- `object`: either a `workflow` or a `model` object
- `preprocessor`: an additional preprocessing recipe or formula expression (only required in case a `model` object is provided)
- `resamples`: a `resamples` object (e.g., our `climbers_folds`)

---

## 4.6 `tune`: Tidy Tuning Tools

The `tune` package unites the previous steps in the context of hyperparameter tuning with the **`tune_grid()`** function being the primary modeling workhorse.
```{r, eval=F}
tune_grid(
  object, preprocessor, resamples,
  grid = 10, metrics = NULL, control = control_grid() #<<
)
```
**Keyword arguments:**
- `grid`: the number of candidate hyperparameter combinations to be tried (defaults to `10` draws from a [Latin hypercube](https://en.wikipedia.org/wiki/Latin_hypercube_sampling)) respectively a pre-defined parameter grid
- `metrics`: a set of performance metrics (defaults to *RMSE* and *R<sup>2</sup>* for regression and *ROC-AUC* and *accuracy* for classification tasks) computed for each resample (customize via `yardstick::metric_set()`) 
- `control`: additional options to control the tuning process (e.g., `save_pred = T` to retain the predictions for each fold or `verbose = T` to print the log)

.footnote[
*Note: Retaining the predictions for each fold can impose a heavy burden on your machine's memory which may become unwieldy if your data set and/or the number of resamples is large.*
]

???
- latin hypercube: space-filling sample algorithm (divide search space into equal cubes, sample from cube)

---

## 4.6 `tune`: Tidy Tuning Tools

To illustrate the entire tuning process, let's again start by first constructing a regularized logit classifier. This time however, we do not fix the hyperparameters, but use the `tune()` placeholder to indicate which hyperparameters are to be optimized.
```{r}
elnet_cls <- logistic_reg() %>%
  set_args(penalty = tune(), mixture = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("glmnet", family = "binomial")
```
Second, we bundle the preprocessing recipe and model specification using the `workflows` package.
```{r}
cls_wf <- workflow() %>% 
  add_recipe(mod_recipe) %>% 
  add_model(elnet_cls)
```

.footnote[
*Note: Tunable hyperparameters (indicated by `tune()`) accept an `id` argument to assign a custom name to each hyperparameter. This may be relevant in cases where you are supposed to tune two hyperparameters which go by the same name. Otherwise, it is usually easier to go by the default name (e.g., `penalty` or `mixture`).*
]

???
- lets assume in this example that we are not sure whether Ridge or the Lasso is more beneficial
- two hyperparameters with same name: e.g., "degrees of freedom" when you want to generate a tunable amount of polynomial terms for more than one predictor

---

## 4.6 `tune`: Tidy Tuning Tools

Third, we create a grid of hyperparameter candidates for performing a random grid search. In general, `dials` offers two alternative ways for creating a set of hyperparameter candidates.

We can either define the grid by providing the `grid_*()` function with one or several `param` objects.
```{r}
param_grid <- grid_regular(penalty(), mixture(), levels = c(10,10))
param_grid %>% 
  tibble::glimpse()
```
Or we extract `param` objects (i.e. hyperparameter flagged with `tune()`) from an already defined preprocessing recipe, model specification or modeling workflow using `parameters()`.
```{r, eval=F}
param_grid <- cls_wf %>% 
  parameters() %>% 
  grid_regular(levels = c(10,10))
```

???
- hyperparameters cannot only be inherent to models, but also to preprocessing steps (e.g., the number of neighbors in `themis::step_smote()`)

---

## 4.6 `tune`: Tidy Tuning Tools

Lastly, we perform hyperparameter tuning using `tune_grid()`. We let the function iterate over all 10 folds included in `climbers_folds`, and evaluate a random grid of 100 candidate hyperparameter pairs for `mixture()` and `penalty()`. This results in 1,000 models being fitted during the procedure.

```{r, eval=F}
start <- Sys.time()

cls_wf_tuned <- tune_grid(
  cls_wf, climbers_folds,
  grid = param_grid,
  metrics = metric_set(roc_auc, accuracy, sens, spec),
  control = control_grid(save_pred = T, verbose = T)
)

Sys.time() - start
```

```
> Time difference of 8.985842 mins
```

--

___

.center[
`r emo::ji("hourglass_flowing_sand")` `r emo::ji("zzz")` `r emo::ji("exploding_head")`

Eventually, the process of hyperparameter tuning can take several minutes, hours or even days - depending on your resample size, the richness of your candidate grid, the model's complexity and your hardware.
]

---

## 4.6 `tune`: Tidy Tuning Tools

**Parallel processing:** Under the hood, `tune` is equipped distributed computing capabilities (which stem from an integration of the [`foreach` package](https://cran.r-project.org/web/packages/foreach/vignettes/foreach.html)). Since the models created during CV are indepedent of each other, it allows to parallelize the grid search algorithm and CV approach across multiple cores.

First, let's detect the number of physical and/or logical cores available for computations using the base `R` `parallel` package.
```{r}
all_cores <- parallel::detectCores(logical = F)
all_cores
```
Second, we create a cluster of `R` sessions running in parallel.
```{r}
comp_cluster <- parallel::makeCluster(all_cores - 2)
comp_cluster
```

???
- **Physical cores** are number of physical cores, actual hardware components.
- **Logical cores** are the number of physical cores times the number of threads that can run on each core through the use of hyper-threading (number of possible parallel simultaneous processes).

- in the background:
  - `tune` divides your data (e.g., resamples) and allocates it across clusters
  - it can be viewed as multiple `R` session running in parallel
  
- tip: its generally a good idea to not use all available clusters
  - oftentimes, performance increases are not linear but you get a rate of diminishing returns
  - the memory load is proportional as copies of your data are transferred to every core
  - you still want to be able to work on your computer while the computations are executed

---

## 4.6 `tune`: Tidy Tuning Tools

Third, we register a backend for parallel computing (here the `doParallel` package), which defines how parallelization is executed, and re-run our tuning procedure - this time with the 250 models being trained on `r all_cores` different cores.

```{r, echo=F, out.height='35%', out.width='35%', out.extra='style="float:right; padding: 30px"'}
knitr::include_graphics("https://tenor.com/view/yay-traffic-car-race-gif-14739892.gif")
```

```{r, eval=F}
doParallel::registerDoParallel(comp_cluster)
start <- Sys.time()

cls_wf_tuned <- tune_grid(
  cls_wf, climbers_folds,
  grid = param_grid,
  metrics = metric_set(roc_auc, accuracy, sens, spec),
  control = control_grid(
    save_pred = T, verbose = T, pkgs = c('themis')
  )
)

Sys.time() - start
```
```
> Time difference of 3.706945 mins
```

.footnote[
*Note: By default, `tidymodels` copies only its core packages to all concurrently running `R` sessions. If you leverage additional packages (e.g., `themis`) as part of your modeling pipeline, it must be provided in the tuning controls.*
]

???
- there are different backends packages that you can use for parallel processing. They start with `do` and vary in the way of how they enable parallel processing.
- as you can see the speed-up is not proportional but diminishing in the number of cores (also somewhat depending on what you are doing in the background)

---

```{r, echo=F}
cls_wf_tuned <- read_rds("./slides/cls_wf_tuned.rds")
```

## 4.6 `tune`: Tidy Tuning Tools

`tune_grid()` updates your initial resample object (here `climbers_folds`) by adding additional columns (`.metrics` and `.notes` plus `.predictions` and others depending on your controls).
```{r}
cls_wf_tuned
```
Now, there are several neat things we can do with our fitted `climbers_folds` data frame. Let's have a look at some selective functions provided by the `tune` package.

???
- note that the resulting tibble does neither include the data (only the indexes) nor the fitted models, but only the performance metrics and predictions (usually we are not interested in the models themselves during resampling, but only in the optimal hyperparameter set)
- .notes captures warnings and errors that occur during execution to help you debugging (i.e. which model and fold potentially produced an error)
- reconfigure this tibble using `tidyr` tools or `tidymodels` convenience functions

---

## 4.6 `tune`: Tidy Tuning Tools

Extract the performance metrics for a single fold.
```{r}
cls_wf_tuned %>% 
  purrr::pluck(".metrics", 1)
```

---

## 4.6 `tune`: Tidy Tuning Tools

Extract the performance metrics for each fold.
```{r}
cls_wf_tuned %>% 
  collect_metrics(summarize = F)
```

???
- 4 metrics * 1,000 models
- the set of collect functions do all the unnesting for you

---

## 4.6 `tune`: Tidy Tuning Tools

Extract the average performance metrics for all folds.
```{r}
cls_wf_tuned %>% 
  collect_metrics(summarize = T)
```

???
now also with uncertainty estimates, i.e. std_err

---

## 4.6 `tune`: Tidy Tuning Tools

Extract the `n` best performing model specifications based on the $AUC$:
```{r}
cls_wf_tuned %>% 
  show_best(metric = "roc_auc", n = 3)
```
Extract the overall best performing model specifications based on the $AUC$:
```{r}
cls_wf_tuned %>% 
  select_best(metric = "roc_auc") # alternatively: select_by_one_std_err(metric = "roc_auc")
```

???
- select best returns the optimal hyperparameter combination which we may then use to construct our final model

---

## 4.6 `tune`: Tidy Tuning Tools

Extract the validation set predictions for each fold and the optimal model specification (only applicable if `control = control_grid(save_pred = TRUE)`).
```{r}
cls_wf_tuned %>% 
  collect_predictions(
    summarize = F,
    parameters = select_best(cls_wf_tuned, metric = "roc_auc")
  )
```

---

## 4.6 `tune`: Tidy Tuning Tools

If we are not concerned about hyperparameter tuning per sÃ©, e.g., because we are training a very simple and explainable model, we can refer to `fit_resamples()` instead of `tune_grid()`.
```{r, eval=F}
elnet_cls <- logistic_reg() %>%
  set_args(penalty = 0.1, mixture = 1) %>% 
  set_mode("classification") %>% 
  set_engine("glmnet", family = "binomial")
```
```{r, eval=F}
cls_wf %>% 
  update_model(elnet_cls) %>% 
  fit_resamples(
    climbers_folds,
    metrics = metric_set(roc_auc, accuracy, sens, spec)
  )
```

???
Note: This approach is only reasonable if your are not supposed to tune any hyperparameters!

---

## 4.6 `tune`: Tidy Tuning Tools

```{r, echo=F, fig.width = 10, fig.asp = 0.618, fig.align='center'}
cls_wf_tuned %>% 
  collect_metrics(summarize = T) %>% 
  ggplot2::ggplot(aes(x = penalty, y = mean, group = mixture, color = mixture)) +
    geom_point() +
    geom_line() +
    scale_x_log10() +
    facet_wrap(~ .metric, scales = "free") +
    theme_light()
```

???
- if we have filled the `metrics` argument in `tune_grid`, we can immediately analyze model performance across the different folds and hyperparameter combinations (here averaged over all 10 folds)

---

## 4.6 `tune`: Tidy Tuning Tools

Since we have now optimized our model for the most promising candidate hyperparameter pair of `mixture` and `penalty`, we can finalize our initial `workflow` object.
```{r, results='hide'}
cls_wf_final <- cls_wf %>% 
  finalize_workflow(select_best(cls_wf_tuned, metric = "roc_auc"))

cls_wf_final
```
```
>  Output on next slide
```

.footnote[
*Note: Would we not have combined our model specification and preprocessing recipe in a `workflow` object, we could alternatively use `finalize_model()` or `finalize_recipe()`.*
]

---

## 4.6 `tune`: Tidy Tuning Tools

```
> == Workflow ===================================================================================
> Preprocessor: Recipe
> Model: logistic_reg()
> 
> -- Preprocessor -------------------------------------------------------------------------------
> 5 Recipe Steps
> * step_medianimpute()   * step_dummy()
> * step_normalize()      * step_smote()
> * step_other()
> 
> -- Model --------------------------------------------------------------------------------------
> Logistic Regression Model Specification (classification)
> 
> Main Arguments:
>   penalty = 0.00274383239650675
>   mixture = 0.0604883791296743
> 
> Engine-Specific Arguments:
>   family = binomial
> 
> Computational engine: glmnet 
```

---

## 4.6 `tune`: Tidy Tuning Tools

Let's retrain our finalized workflow on the whole training set and predict death rates for the unseen data.
```{r, eval=F}
cls_wf_final %>% 
  fit(data = train_set) %>% 
  predict(new_data = test_set, type = "prob")
```

--

`tune` streamlines this step and introduces the `last_fit()` function which employs the initial `split` object we generated in [chapter 4.1](#data-split) as input. More precisely, it fits the finalized workflow on the training set, predicts the unseen cases in the test set and evaluates the model's performance by benchmarking predictions against the actual response.
```{r}
cls_wf_results <- cls_wf_final %>% 
  last_fit(split = climbers_split, metrics = metric_set(roc_auc, accuracy, sens, spec))

cls_wf_results
```

---

## 4.6 `tune`: Tidy Tuning Tools

We have now successfully tuned a single machine learning model! `r emo::ji("hugs")` `r emo::ji("star_struck")`
<br><br>

--

Eventually, however, we would like multiple models to compete on a given task and choose the winner. `r emo::ji("1st_place_medal")`
<br><br>

--

**Kuhn/Johnson (2013) framework for model selection [[2, p. 79]](#references):** 
1. Start with very flexible *black-box* models (e.g., boosted trees or SVM) to produce an optimal benchmark (*performance ceiling*).
2. Evaluate slightly less opaque models which provide a baseline degree of interpretability (e.g., PLS, PCA or regularized regression).
3. Try out a parsimonious *white-box* model (e.g., linear regression or CART) and investigate if it can reasonably approximate the performance ceiling.

.footnote[
_Note: For a comprehensive overview of the topic of *interpretable ML* check out [this book](https://christophm.github.io/interpretable-ml-book/) by Christoph Mulner. [[3](#references)]_
]

???
- your goal is to find the simplest possible model with reasonable performance
