---
title: "04_model_building"
output: html_document
---

layout: false
class: middle, center, inverse

# 4.3 `parsnip`:<br><br>A Common API to Modeling and Analysis Functions

---

background-image: url(https://www.tidymodels.org/images/parsnip.png)
background-position: 97.5% 5%
background-size: 7%
layout: true

---

## 4.3 `parsnip`: A Unified Modeling API

.pull-left[
So far, you have worked with a heterogeneous set of packages to implement your models:

`stats::lm`, `stats::glm`, `MASS::lda`, `class::knn`, `glmnet::glmnet`, `rpart::rpart`, `gbm::gbm`, `randomForest::randomForest`, `e1071::svm`, ...

Likely, you have struggled with the varying naming conventions, interfaces and syntactical intricacies of each package.
]
.pull-right[
```{r, echo=F, out.height='60%', out.width='60%', fig.align='center'}
knitr::include_graphics("https://tenor.com/view/ballin-juggling-talent-juggle-wow-gif-16262578.gif")
```
]

--

.center[The same holds for implementations of one and the same model by different packages:]
```{r, echo=F, fig.align='center'}
tibble(
  'Argument' = c("Number of predictors", "Number of trees", "Number of split points"),
  'randomForest' = c("mtry", "ntree", "nodesize"),
  'ranger' = c("mtry", " num.trees", "min.node.size"),
  'sparklyr' = c("feature_subset_strategy", "num_trees", "min_instances_per_node")
) %>% 
  knitr::kable()
```

???
- spark is an alternative programming language
- often machine learning methods are implemented in C++ due to superior speed and a R frontend is put on top of the C++ implementation
- note: this heterogeneity can be observed across the whole modeling landscape in R (e.g., also each package has its own `predict()` functions with slightly differing naming conventions)

---

## 4.3 `parsnip`: A Unified Modeling API

.pull-left[
`parsnip` provides a **unified interface** and syntax to modeling which facilitates your overall modeling workflow. The goals of `parsnip` are twofold:
1. Decoupling model definition from model fitting and model evaluation<br><br>
2. Harmonizing function arguments (e.g., `ntree`, `num.trees` and `num_trees` become `trees` or `k` becomes `neighbors`)
]
.pull-right[
```{r, echo=F, out.height='65%', out.width='65%', fig.align='center'}
knitr::include_graphics("https://tenor.com/view/balls-rolling-racing-rolling-on-ball-yoga-balls-gif-15365855.gif")
```
]

???
- the goal is to make function arguments more expressive (`neighbor` instead of `k`, `penalty` instead of `lambda`)
- in `parsnip`: `trees`

---

## 4.3 `parsnip`: A Unified Modeling API

```{r, echo=F, out.height='60%', out.width='60%', out.extra='style="float:right; padding:10px"'}
knitr::include_graphics("https://raw.githubusercontent.com/allisonhorst/stats-illustrations/master/rstats-artwork/parsnip.png")
```

In `parsnip` a model is always made up of three individual components:
- **Type:** The model type that is about to be fitted (e.g., linear/logit regression, random forest or SVM).<br><br>
- **Mode:** The mode of prediction, i.e. regression or classification.<br><br>
- **Engine:** The computational engine implemented in `R` which usually corresponds to a certain package (e.g., `rpart`, `glm`, `randomForest`) or computing framework (e.g., `Stan`, `sparklyr`).

Check all models and engines supported by `parsnip` on the [`tidymodels` website](https://www.tidymodels.org/find/parsnip/).

---

## 4.3 `parsnip`: A Unified Modeling API

**Logistic classifier:**
```{r}
log_cls <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

log_cls
```

???
- note that some model families support both modes, some only one of the two (e.g., LDA only for classification, ARIMA models only for regressions)
- note that we did not reference the data in any way so far (variable roles are entirely specified by our recipe)

---

## 4.3 `parsnip`: A Unified Modeling API

**Regularized logistic classifier:**
```{r}
lasso_cls <- logistic_reg() %>%
  set_args(penalty = 0.1, mixture = 1) %>% 
  set_mode("classification") %>% 
  set_engine("glmnet", family = "binomial")

lasso_cls
```

.footnote[
*Note: `parsnip` distinguishes between model arguments (frequently used across engines) and engine arguments (rarely used and/or only apply to specific engines).*
]

???
- the function arguments could also be specified directly in the model function, but this way it is more transparent and sequential
- mixture reflects the amount of the l1 respectively l2 penalty

---

## 4.3 `parsnip`: A Unified Modeling API

**Decision tree classifier:**
```{r}
dt_cls <- decision_tree() %>% 
  set_args(cost_complexity = 0.01, tree_depth = 30, min_n = 20) %>% 
  set_mode("classification") %>% 
  set_engine("rpart")

dt_cls
```

.footnote[
*Note: If not explicitly specified, `parsnip` adopts the model's default parameters (i.e. function arguments) defined by the underlying engine (here `rpart`).*
]

---

## 4.3 `parsnip`: A Unified Modeling API

**Tree bagging classifier:**
```{r}
rand_forest() %>% 
  set_args(trees = 1000) %>% 
  set_mode("classification") %>% 
  set_engine("randomForest") %>% 
  translate()
```

.footnote[
*Note: Apply `translate()` to investigate how `parsnip` translates the specification into the underlying computational engine.*
]

---

## 4.3 `parsnip`: A Unified Modeling API

**Random forest classifier:**
```{r}
rand_forest() %>%
  set_args(trees = 1000, mtry = .cols()) %>% 
  set_mode("classification") %>% 
  set_engine("randomForest")
```

.footnote[
*Note: Use data set characteristics as placeholder arguments which reflect the number of predictors in your data set. `.preds()` and `.cols()` capture the number of predictors in your data prior respectively subsequent to feature engineering (e.g., one-hot encoding).*
]

---

## 4.3 `parsnip`: A Unified Modeling API

**k-nearest-neighbor classifier:**
```{r}
nearest_neighbor() %>% 
  set_args(neighbors = 5, dist_power = 2) %>% 
  set_mode("classification") %>% 
  set_engine("kknn")
```

???
- dist_power: 1 (manhattan), 2 (euclidean)

---

## 4.3 `parsnip`: A Unified Modeling API

**SVM classifier:**
```{r}
svm_rbf() %>% 
  set_args(cost = tune(), rbf_sigma = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("kernlab")
```

.footnote[
*Note: Use the `tune()` placeholder as a model argument when the parameter is supposed to be specified later on in the workflow (e.g., during hyperparameter tuning).*
]

---

## 4.3 `parsnip`: A Unified Modeling API

Finally, it is time to fit our specified model to the training data. Since some modeling functions require a formula (e.g., `lm()`) as input and others a vector, a matrix (e.g., `glmnet()`) or a data frame, `parsnip` offers two modes for fitting.

**Formula-based interface:**
```{r, eval=T}
dt_cls_fit <- dt_cls %>% 
  fit(formula = died ~ ., data = train_set)
```
**Non-formula-based interface:**
```{r, eval=T}
dt_cls_fit <- dt_cls %>% 
  fit_xy(x = train_set %>% select(-died), y = train_set$died)
```

.footnote[
*Note: To accommodate different user preferences, `parsnip` allows you to use any of the two interfaces independent of the requirements of the underlying engine. Bare in mind, however, that only the formula notation automatically creates dummies whereas `fit_xy()` takes the data as-is.*
]

---

## 4.3 `parsnip`: A Unified Modeling API

.pull-left[
After fitting the model, we can eventually predict the response in the test data.
```{r}
dt_cls_fit %>% 
  predict(new_data = test_set, type = "prob")
```
]

--

.pull-right[
**`tidymodels` prediction rules:** [[1, ch. 7.3](#references)]
1. Predictions are returned as a `tibble` (no need to extract predictions from an object).<br><br>
2. Column names are predictable (`.pred`, `.pred_class`, `.pred_lower`/`.pred_upper`, etc. depending on the prediction `type`).<br><br>
3. The number of predictions equals the number of data points in `new_data` (and is in the same order).
]

???
- leading dots protect against merging errors based on identical column names

---

## 4.3 `parsnip`: A Unified Modeling API

Thanks to these rules, we can directly combine the predictions with the `test_set`.
```{r}
test_set %>% 
  bind_cols(predict(dt_cls_fit, new_data = ., type = "prob")) %>% 
  slice_head(n = 3)
```

--

___

.center[
`r emo::ji("warning")` **Notice that we did not apply any of our predefined feature engineering steps!** `r emo::ji("warning")`

*The code will throw an error if we try to fit any of our logit models due to the absence of dummies.  
Besides, the Lasso model would perform poorly due to the differently scaled predictors.  
Likewise, the tree model will always predict the negative class due to the severe class imbalance.*
]
