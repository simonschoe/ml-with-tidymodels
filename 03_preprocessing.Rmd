---
title: "03_preprocessing"
output: html_document
---

layout: false
class: middle, center, inverse

# 4.2 `recipes`:<br><br>Preprocessing Tools to Create Design Matrices

---

background-image: url(https://www.tidymodels.org/images/recipes.png)
background-position: 97.5% 5%
background-size: 7%
layout: true

---

## 4.2 `recipes`: Preprocessing Tools

> In statistics, a **design matrix** (also known as **regressor matrix** or **model matrix**) is a matrix of values of explanatory variables of a set of objects, often denoted by $X$. Each row represents an individual object, with the successive columns corresponding to the variables and their specific values for that object. ~ [Wikipedia](https://en.wikipedia.org/wiki/Design_matrix)

--

Essentially, every model in `R` requires a design matrix as input. Intuitively, we can think of a design or model matrix as a tidy data frame (with one observation per row and one predictor per column) which can be directly processed by the model function.

--

<br>

Oftentimes, however, data frames or matrices that we apply to a model function do not come in the required format. This boils down to the issue of *feature engineering*. For example:
- A linear model (`lm`) requires categorical predictors to be one-hot encoded into $C-1$ binary predictors.
- A decision tree (`rpart`) does not require strictly numerical predictors.
- A support vector machine (`svm`) performs best with scaled predictors.
- And numerous models reject missing values in their model matrix.

.footnote[
*Note: Some functions internally convert a data frame to a numeric design matrix (e.g., `lm()` automatically one-hot encodes unordered factors and creates polynomial contrasts from ordered factors).*
]

???
Most R functions create the design matrix automatically from a given data frame according to the formula that is provided in the function call.

---

## 4.2 `recipes`: Preprocessing Tools

The `recipes` package provides functions for defining a **blueprint for feature engineering**. Each `recipe` is constructed by sequentially chaining different preprocessing steps.

--

First, create a `recipe` object from your data using the `recipe()` function and two arguments:
- **`formula`:** A formula to declare variable roles, i.e. everything on the left-hand side (LHS) of the `~` is declared as `outcome` and everything on the right-hand side (RHS) as `predictor`.
- **`data`:** The data to which the feature engineering steps are later applied. The data set is only used to catalogue the variables and their respective types (which is why you generally provide the training set).
```{r}
mod_recipe <- recipe(formula = died ~ ., data = train_set)
mod_recipe
```

---

## 4.2 `recipes`: Preprocessing Tools

Second, we add new steps (`step_*()`) to the recipe in order to declare feature engineering steps:

1. Use `update_role()` to assign a new custom role to a predictor. As `member_id` simply enumerates our observations, it is assigned the `"id"` role and hence not considered in any downstream modeling task.<br><br>
2. Use `step_medianimpute()` to impute `NA` values by the median predictor value. Since roughly 3,500 missing values are inherent to `age`, we use median-imputation to retain those observations.<br><br>
3. Use `step_normalize()` to scale numerical data to zero mean and unit standard deviation (which is required for scale-sensitive classifiers).<br><br>
4. Use `step_other()` to lump together rarely occurring factor levels. `peak_name`, `citizenship` and `expedition_role` all have several 100 factor levels and hence a high risk of being near-zero variance features. All factor levels with a relative frequency below 5% are pooled into `"other"`.<br><br>
5. Use `step_dummy()` to one-hot encode categorical predictors.<br><br>
6. Finally, we use `step_smote()` from the `themis` package to tackle class imbalance. Synthetic samples are generated via 5-NN to establish a class distribution ratio of 3:5.

---

## 4.2 `recipes`: Preprocessing Tools

```{r}
mod_recipe <- mod_recipe %>% 
  update_role(member_id, new_role = "id")

mod_recipe
```

.footnote[
<i>
Note: Change the role of a predictor to keep it in the data, however, without being used during model fitting.

Usually `step_*()` functions do not change the role of a predictor. However, each `step_*()` function contains a `role` argument to explicitly specify the role of a newly generated predictor.
</i>
]

???
- with the `new_role` argument I can set any custom role name

---

## 4.2 `recipes`: Preprocessing Tools

```{r}
mod_recipe <- mod_recipe %>% 
  step_medianimpute(age)

mod_recipe
```

???
- essence of recipes: the steps are only declared and not directly executed!

---

## 4.2 `recipes`: Preprocessing Tools

```{r}
mod_recipe <- mod_recipe %>% 
  step_normalize(all_numeric())

mod_recipe
```

.footnote[
*Note: Variables can be selected by referring either to their name, their data type, their role (as specified by the recipe) or by using the `select()` helpers from `dplyr` (e.g., `contains()`, `starts_with()`).*
]

---

## 4.2 `recipes`: Preprocessing Tools

```{r}
mod_recipe <- mod_recipe %>% 
  step_other(peak_name, citizenship, expedition_role, threshold = 0.05)

mod_recipe
```

.footnote[
*Note: You should always take care of the order of your steps. For example, you should first lump together factor levels and then create dummies. Otherwise the `recipe` would generate a large amount of near-zero variance dummies.*
]

???
same holds for the normalize steps which should follow the median-impute step.

---

## 4.2 `recipes`: Preprocessing Tools

```{r}
mod_recipe <- mod_recipe %>% 
  step_dummy(all_predictors(), -all_numeric())

mod_recipe
```

.footnote[
*Note: Use `one_hot = T` in case you want to retain all $C$ factor levels instead of just $C-1$.*
]

---

## 4.2 `recipes`: Preprocessing Tools

```{r, message=F, warning=F}
mod_recipe <- mod_recipe %>% 
  themis::step_smote(died, over_ratio = 0.6, neighbors = 5, seed = 2020, skip = T)

mod_recipe
```

.footnote[
<i>
Note: Each `step_*()` function contains a `skip` argument which is mostly equal to `FALSE` by default. Yet, for certain preprocessing steps (e.g., under- or oversampling) we set it to `TRUE` in order to not apply it to the test set and hence retain its original properties.
</i>
]

???
- Usually, you would want to chain the steps together instead of defining each step separately (here its only done for presentation purposes)

---

layout: false

## Excursus: Imperative vs. Declarative Programming

Up to this point, you have not performed any actual transformation of your data - you have only sketched a blueprint of what `R` is theoretically supposed to do with your data. The difference between instantly executing a command and declaring it, in case it is prospectively needed, relates to two important programming paradigms [[1]](https://mastering-shiny.org/basic-reactivity.html#imperative-vs-declarative-programming):
- **Imperative programming:** A command is entered and immediately executed (what you are likely used to do in `R` so far).<br><br>
- **Declarative programming:** A command is specified, along with some important constraints, however, the execution of the code occurs at later point in time, either specified by the user or the program (which you have to get used to when working with machine learning tools, e.g., `tidymodels`).

---

background-image: url(https://www.tidymodels.org/images/recipes.png)
background-position: 97.5% 5%
background-size: 7%
layout: true

---

## 4.2 `recipes`: Preprocessing Tools

Third, `prep()` fits the recipe to estimate the unknown quantities (e.g., medians or pooled factor levels).
```{r}
mod_recipe_prepped <- prep(mod_recipe, retain = T)
mod_recipe_prepped
```

.pull-right[.pull-right[.footnote[
*Note: By applying `prep()` to the final recipe, we fit the recipe only to the training set (as specified in the `recipe()` function above). Thus, we prevent the issue of data leakage!*
]]]

???
- other unknown quantities: means and sd for scaling, new data points via SMOTE
- retain saves the preprocessed data set (here the training data)
  - do this to avoid unnecessary recomputation each time you fit a model
  - don't do this if your working with really big data
- see in the output that the steps are now `[trained]`; output also shows results of the selectors

---

## 4.2 `recipes`: Preprocessing Tools

Fourth, we can finally apply the fitted `recipe` to our data and perform the feature engineering steps.
```{r eval=F}
bake(mod_recipe_prepped, new_data = NULL)
```
```{r, echo=F}
print(bake(mod_recipe_prepped, new_data = NULL), n = 5)
```

.footnote[
*Note: Set `new_data = NULL` to apply the `recipe` to the data set provided to `recipe()`, i.e. the training set. Set `new_data = test_set` instead, if it should be applied to the test set.*
]

???
Note:
- dummy encodings worked
- encoding `other` category worked
- smote worked (96,470 vs. 61,177 samples in the original train_set)

- can also provide a selector to `bake` if your only interested in some of the transformed predictors
- up to this point we have never used the test set, i.e. we can be sure that data leakage is absent
- when `new_data = test_set` is does not re-estimate the quantities (e.g., mean, median) since they are drawn from the recipe that is estimated on the training data

---

## 4.2 `recipes`: Preprocessing Tools

```{r, echo=F, out.height='65%', out.width='65%', out.extra='style="float:right; padding=10px"'}
knitr::include_graphics("https://raw.githubusercontent.com/allisonhorst/stats-illustrations/master/rstats-artwork/recipes.png")
```

**Benefits of using recipes:** [[1, ch. 6.1](#references)]
- Recycle feature engineering blueprint across multiple model candidates.<br><br>
- Extended scope of feature engineering relative to the use of formula expressions (`y ~ x`).<br><br>
- Compact syntax due to the various selector helpers.<br><br>
- The feature engineering pipeline is encapsulated into a single object and scattered throught the `R` script.

---

## 4.2 `recipes`: Preprocessing Tools

Altogether, the `recipes` package offers a variety of built-in preprocessing steps:
```{r, include=F}
grep("^step_", ls("package:recipes"), value = TRUE)  
```
```
>  [1] "step_arrange"       "step_bagimpute"     "step_bin2factor"    "step_BoxCox"        "step_bs"           
>  [6] "step_center"        "step_classdist"     "step_corr"          "step_count"         "step_cut"          
> [11] "step_date"          "step_depth"         "step_discretize"    "step_downsample"    "step_dummy"        
> [16] "step_factor2string" "step_filter"        "step_geodist"       "step_holiday"       "step_hyperbolic"   
> [21] "step_ica"           "step_impute_linear" "step_integer"       "step_interact"      "step_intercept"    
> [26] "step_inverse"       "step_invlogit"      "step_isomap"        "step_knnimpute"     "step_kpca"         
> [31] "step_kpca_poly"     "step_kpca_rbf"      "step_lag"           "step_lincomb"       "step_log"          
> [36] "step_logit"         "step_lowerimpute"   "step_meanimpute"    "step_medianimpute"  "step_modeimpute"   
> [41] "step_mutate"        "step_mutate_at"     "step_naomit"        "step_nnmf"          "step_normalize"    
> [46] "step_novel"         "step_ns"            "step_num2factor"    "step_nzv"           "step_ordinalscore" 
> [51] "step_other"         "step_pca"           "step_pls"           ...
```

In addition, you may also include checks in your pipeline to test for a specific condition of your variables:
```{r, include=F}
grep("check_", ls("package:recipes"), value = TRUE)  
```
```
> [1] "check_class"      "check_cols"       "check_missing"    "check_name"       "check_new_values"
> [6] "check_range"      "check_type"
```

.footnote[
*Note: Learn more about the capabilities of `recipes` in Kuhn/Silge (2020) [[1, ch. 6](#references)], alongside recommended preprocessing operations for each model type. [[1, Appendix A](#references)]*
]

???
- `step_date`: converts a date into factor variables, e.g., day of the week or month
- `step_holiday`: creates a dummy for a national holiday
- `step_corr`: removes variables that have large absolute correlations with other variables
- `step_normalize`: applies z-Transformation to predictors
- `step_mutate`: to engineer new variables (analogue to `dplyr`)
- `step_ordinalscore`: map an ordered factor to integer values
- `step_interact`: to create interaction effects

-> basically, all transformation steps you would do using dplyr before modeling, respectively all steps that the model formula would enforce can be embedded as a recipe step within your modeling workflow
