---
title: "01_intro"
output: html_document
---

layout: false
class: middle, center, inverse

# 4.7 `broom`:<br><br>Convert Statistical Objects into Tidy Tibbles

---

background-image: url(https://www.tidymodels.org/images/broom.png)
background-position: 95% 5%
background-size: 7.5%
layout: true

---

## 4.7 `broom`: Tidy Model Outputs

`broom` provides three useful functions for converting model objects (e.g., `lm`, `glm`, `rpart`) into tidy `tibbles`:
- `tidy()`: produces a tidy output of model components (e.g., coefficients, weights, clusters)
- `glance()`: produces a tidy output of model summaries (e.g., goodness-of-fit, $F$-statistics)
- `augment()`: adds additional information about observations (e.g., fitted values, residuals)

```{r, echo=F, out.width='60%', fig.align='center'}
knitr::include_graphics("https://raw.githubusercontent.com/allisonhorst/stats-illustrations/master/rstats-artwork/broom_package.png")
```

???
- most of the columns `tidymodels` creates have the "." prefix in order to not override initial columns
- difference to `tidyr`: these functions tidy model objects, `tidyr` is all about tidying and transforming data frames

---

## 4.7 `broom`: Tidy Model Outputs

In order to illustrate the convenience of the three `broom` functions, let us first extract our optimal model from `cls_wf_results`.
```{r, results='hide'}
elnet_cls_final <- cls_wf_results %>% 
  pluck(".workflow", 1) %>% 
  pull_workflow_fit()

elnet_cls_final
```
```
> parsnip model object
> 
> Fit time:  4.8s 
> 
> Call:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = ~"binomial", alpha = > ~0.111111111111111) 
> 
>    Df  %Dev  Lambda
> 1   0  0.00 1.09200
> 2   1  0.17 0.99480
> 3   1  0.36 0.90640
> 4   2  0.56 0.82590
> 5   2  0.86 0.75250
> 6   2  1.16 0.68560
> ...
```

???
- we see the `call`, i.e. how `tune` has translated our workflow into the original engines
- alpha is set to 0.1111 according to our hyperparameter tuning procedure
- non-tidy output:
  - `Df`: number of non-zero coefs
  - `%Dev`: a measure for the explained variation
  - `lambda`: `penalty` hyperparameter

---

## 4.7 `broom`: Tidy Model Outputs

.pull-left[
**`tidy()`:** produces a tidy output of model components (e.g., coefficients, weights, clusters)
```{r, eval=F}
tidy(elnet_cls_final)
```
```{r, echo=F}
print(tidy(elnet_cls_final), n = 8)
```
]
.pull-right[
**`glance()`:** produces a tidy output of model diagnostics (e.g., goodness-of-fit, F-statistics)
```{r}
glance(elnet_cls_final)
```

**`augment()`:** adds additional information about observations (e.g., fitted values, residuals)

Unfortunately, `augment()` is not supported for `glmnet` models (check [available methods](https://broom.tidymodels.org/articles/available-methods.html)).
]

.footnote[
*Note: Depending on the class of the model object you are providing to `tidy()`, it offers several advanced features, such as returning odds-ratios for logit-models (`exponentiate = T`) or confidence interval (`conf.int = T`).*
]

???
- `tidy`: useful for creating visualizations or preparing model tables for a paper
- `glance`: useful for investigating overall model performance, identify misspecifications or compare models
- these functions are implemented with the need of data scientist in mind, i.e. what are the statistics the modeler is most likely interested in?
- they also work with techniques from classical statistics such as t-tests

---

layout: false
class: middle, center, inverse

# 4.8 `yardstick`:<br><br>Tidy Characterizations of Model Performance

---

background-image: url(https://www.tidymodels.org/images/yardstick.png)
background-position: 95% 5%
background-size: 7.5%
layout: true

---

## 4.8 `yardstick`: Tidy Model Performance

Similar to `broom`, `yardstick`'s endeavor is to enable model evaluation using *tidy data principles*. It ships with three types of metrics which can be distinguished by their input arguments and use case:

**Classification:**
- **Class metrics** are based on the predicted class (*hard predictions*) and take two `fct` columns (`truth` and `estimate`).
- **Class probability metrics** are based on the predicted probabilities (*soft predictions*) and take one `fct` column (`truth`) and one/multiple `dbl` columns containing the class probabilities (`estimate`).

**Regression:**
- **Numeric metrics** are based on a numerical prediction and take two `dbl` columns (`truth` and `estimate`).

.footnote[
*Note: Find all available metrics grouped by their type on [tidymodels.org](https://yardstick.tidymodels.org/articles/metric-types.html#metrics).*
]

---

## 4.8 `yardstick`: Tidy Model Performance

In order to compute various *class* and *class probability metrics* for our regularized logistic classifier, we must first extract the `.predictions` from our optimal fitted model.
```{r}
cls_wf_results <- cls_wf_results %>% 
  purrr::pluck(".predictions", 1) 

cls_wf_results
```

---

## 4.8 `yardstick`: Tidy Model Performance

.pull-left[
**Class metrics:** `conf_mat()`
```{r}
cls_wf_results %>% 
  conf_mat(died, estimate = .pred_class)
```
**Class metrics:** `sens()` (sensitivity)
```{r, eval=F}
cls_wf_results %>% 
  sens(died, estimate = .pred_class)
```
**Class metrics:** `spec()` (specificity)
```{r, eval=F}
cls_wf_results %>% 
  spec(died, estimate = .pred_class)
```
]
.pull-right[
**Class metrics:** `accuracy()`
```{r, eval=F}
cls_wf_results %>% 
  accuracy(died, estimate = .pred_class)
```

Finally, you may also create a `metric_set` to compute multiple performance metrics at once.
```{r}
metrics <- metric_set(accuracy, sens, spec)

metrics(
  cls_wf_results, 
  died, estimate = .pred_class
)
```
]

???
- "binary" indicates that the measures are computed for a binary classification problem
- for multi-class problems `yardstick` has implemented generalizations of the original measures

---

## 4.8 `yardstick`: Tidy Model Performance

.pull-left[
**Class probability metrics:** `roc_curve()`
```{r}
cls_wf_results %>% 
  roc_curve(
    .pred_TRUE, truth = died,
    event_level = "second"
  )
```
]
.pull-right[
**Class probability metrics:** `roc_auc()`
```{r}
cls_wf_results %>% 
  roc_auc(
    .pred_TRUE, truth = died,
    event_level = "second"
  )
```
]

.footnote[.pull-right[
*Note: By default, `yardstick` views the first factor level as the positive class. If your outcome is one-hot encoded (e.g., `0`/`1` or `FALSE`/`TRUE`) and the event of interest relates to the second factor level, you have to make the `event_level` explicit.*
]]

???
- class probability metrics: we provide the probability column for the event of interest

---

## 4.8 `yardstick`: Tidy Model Performance

The individual functions are not only applicable to our final test set predictions for the optimal model, but work equally well on a `tibble` of predictions, grouped by resample.

.pull-left[
```{r roc_curves, eval=F}
cls_wf_tuned %>% 
  collect_predictions() %>% 
  group_by(id) %>% 
  roc_curve(
    died, .pred_TRUE,
    event_level = "second"
  ) %>% 
  autoplot()
```
]
.pull-right[
```{r, ref.label='roc_curves', echo=F, fig.width = 10, fig.asp = 0.618, fig.align='center'}
```
]

.footnote[
*Note: Learn more about the `yardstick` package, e.g., features in the context of multi-class learning problems, in Kuhn/Silge (2020). [[1, ch. 9](#references)]*
]

???
- yardsticks functions have a consistent API that allows to easily operate on grouped data
- ideally you would not only want to compare the model performance for different folds but also across models -> this is straightforward with `yardstick` as well

---

layout: false

## Excursus: Alternative Evaluation Dimensions

- **Scalability:** How well does the model scale to larger data sets? Is the speed of model re-training and prediction adversely affected in a real-time scenario?<br><br>
- **Robustness**: Is the model robust against perturbations (e.g., missing values or outliers) in the unseen data? Does the performance deteriorate rapidly in the context of data drift?<br><br>
- **Transferability:** Can the model be applied to related tasks without substantial re-training of the model and without a substantial loss of predictive accuracy?<br><br>
- **Interpretability:** Are predictions explainable? Can the relationship between a predictor and the outcome be extracted from the model?<br><br>
- **Fairness & Compliance:** Does the model systematically discriminate against certain sub-populations or ethical groups? Does it comply with prevalent law in a given domain?<br><br>
- **Justifiability:** Are the predictions in line with well-known business rules? Are the most important predictors consistent with prior beliefs?<br><br>
- **Causality:** Does the model allow causal inference? If no, does it enable the user to generate hypotheses that can be tested using alternative statistical approaches?<br><br>
