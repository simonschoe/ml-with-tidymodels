<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Machine Learning in R: Workshop Series</title>
    <meta charset="utf-8" />
    <meta name="author" content="Simon Sch√∂lzel" />
    <meta name="date" content="2020-12-14" />
    <link href="libs/panelset-0.2.4/panelset.css" rel="stylesheet" />
    <script src="libs/panelset-0.2.4/panelset.js"></script>
    <link href="libs/xaringanExtra-extra-styles-0.2.4/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <meta name="github-repo" content="simonschoe/ml-with-tidymodels"/>
    <meta name="twitter:title" content="Modeling Workflows with Tidymodels"/>
    <meta name="twitter:description" content="4 hour workshop on tidymodels, a unified framework towards modeling and machine learning in R using tidy data principles."/>
    <meta name="twitter:url" content="https://simonschoe.github.io/ml-with-tidymodels"/>
    <meta name="twitter:image:src" content=""/>
    <meta name="twitter:image:alt" content="Title slide for Modeling Workflows with Tidymodels"/>
    <meta name="twitter:card" content="summary_large_image"/>
    <meta property="og:title" content="Modeling Workflows with Tidymodels"/>
    <meta property="og:description" content="4 hour workshop on tidymodels, a unified framework towards modeling and machine learning in R using tidy data principles."/>
    <meta property="og:url" content="https://simonschoe.github.io/ml-with-tidymodels"/>
    <meta property="og:image" content=""/>
    <meta property="og:image:alt" content="Title slide for Modeling Workflows with Tidymodels"/>
    <meta property="og:type" content="website"/>
    <meta property="og:locale" content="en_US"/>
    <meta property="article:author" content="Simon Sch√∂lzel"/>
    <link rel="stylesheet" href="custom/xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="custom/custom-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">




class: center, middle, hide-count
count: false

# Machine Learning in R:&lt;br/&gt;Workshop Series
### Modeling Workflows with Tidymodels

___

**Simon Sch√∂lzel**

2020-12-14

&lt;br&gt;&lt;br&gt;

&lt;a href="https://www.wiwi.uni-muenster.de/"&gt;&lt;img src="https://www.wiwi.uni-muenster.de/fakultaet/sites/all/themes/wwucd/assets/images/logos/secondary_wiwi_aacsb_german.jpg" alt="fb4-logo" height="45"&gt;&lt;/a&gt; &lt;a href="https://www.wiwi.uni-muenster.de/ctrl/aktuelles"&gt;&lt;img src="https://www.wiwi.uni-muenster.de/ctrl/sites/all/themes/wwucd/assets/images/logos/berenslogo5.jpg" alt="ftb-logo" height="45"&gt;&lt;/a&gt; &lt;a href="https://www.wiwi.uni-muenster.de/iff2/de/news"&gt;&lt;img src="https://www.wiwi.uni-muenster.de/iff2/sites/all/themes/wwucd/assets/images/logos/logo_iff2_en2.jpg" alt="ipb-logo" height="45"&gt;&lt;/a&gt;

---

name: agenda

## Agenda

**1 Learning Objectives**

**2 Introduction to `tidymodels`**

**3 Himalayan Climbing Expeditions Data**

**4 The Core `tidymodels` Packages**

&gt;4.1 `rsample`: General Resampling Infrastructure  
4.2 `recipes`: Preprocessing Tools to Create Design Matrices  
4.3 `parsnip`: A Common API to Modeling and Analysis Functions  
4.4 `workflows`: Modeling Workflows  
4.5 `dials`: Tools for Creating Tuning Parameter Values  
4.6 `tune`: Tidy Tuning Tools  
4.7 `broom`: Convert Statistical Objects into Tidy Tibbles  
4.8 `yardstick`: Tidy Characterizations of Model Performance

**5 Additions to the `tidymodels` Ecosystem**

---

## 1 Learning Objectives üí°

This workshop introduces `tidymodels`, a unified framework towards modeling and machine learning in `R` using tidy data principles. You will get to know tools that facilitate every step of your machine learning workflow, from resampling, over feature engineering and model building, to model tuning and performance evaluation.

More specifically, after this workshop you will
- be familiar with the core packages of the `tidymodels` ecosystem and hopefully realize the value of a unified modeling framework,&lt;br&gt;&lt;br&gt;
- know how to design a full-fledged machine learning pipeline for a particular prediction task,&lt;br&gt;&lt;br&gt;
- broaden your technical skill set by learning about declarative programming, hyperparameter scales and parallel processing, and&lt;br&gt;&lt;br&gt;
- most importantly, be capable of conducting your own machine learning projects in `R`.

---



class: middle, center, inverse

# 2 Introduction to `tidymodels`

---

background-image: url(https://www.tidymodels.org/images/tidymodels.png)
background-position: 97.5% 5%
background-size: 7%
layout: true

---

## 2 Introduction to `tidymodels`

&gt; The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles. ~ [tidymodels.org](https://www.tidymodels.org/)

.pull-left[.center[
&lt;img src="https://raw.githubusercontent.com/tidymodels/tidymodels/master/tidymodels_hex.png" width="40%" height="40%" /&gt;

Official `tidymodels` [Hex Sticker](https://github.com/rstudio/hex-stickers)
]]

.pull-right[
.pull-left[
&lt;img src="https://pbs.twimg.com/profile_images/905186381995147264/7zKAG5sY.jpg" width="80%" height="80%" /&gt;

**Hadley Wickham** - Chief Scientist @ RStudio 
]
.pull-right[
&lt;img src="https://avatars3.githubusercontent.com/u/5731043?s=460&amp;u=814e99b02976f8bcb9b15047777742d268648d35&amp;v=4" width="80%" height="80%" /&gt;

**Max Kuhn** - Software Engineer @ RStudio 
]]

--

&gt; Whenever possible, the software should be able to protect users from committing mistakes. Software should make it easy for users to do the right thing. ~ [Kuhn/Silge (2020), ch. 1](#references)

???
- a framework for modeling (guardrails) using using tidy data principles
- very similar to the unified `scikit-learn` package in the context of `Python`
- by the way, this is general a central distinction between R and Python: Python advocates the paradigm of having one unified approach for every problem (which makes it at times also less flexible)

---

## 2 Introduction to `tidymodels`

&gt; The tidymodels framework is a **collection of packages** for modeling and machine learning using tidyverse principles. ~ [tidymodels.org](https://www.tidymodels.org/)

.pull-left[
**`tidymodels` core packages along the generic machine learning pipeline:**
- `rsample`: general methods for resampling
- `recipes`: unified interface to data preprocessing
- `parsnip`: unified interface to modeling
- `workflows`: combine model blueprints and preprocessing recipes
- `dials`: create tuning parameters
- `tune`: hyperparameter tuning
- `broom`: tidy model outputs
- `yardstick`: model evaluation
]
.pull-right[
&lt;img src="./img/tidymodels-hex.PNG" width="85%" height="85%" style="display: block; margin: auto;" /&gt;
]

???
- tidymodels can be viewed as another meta-package that shares the design philosophy, grammar and data structures of the tidyverse
- each package has its own goal which makes tidymodels a modular collection of package
- A goal of the tidymodels packages is that the interfaces to common tasks are standardized
- we will discuss each package along the modeling workflow: resampling, preprocessing, model building, hyperparameter tuning, model evaluation

---

## 2 Introduction to `tidymodels`

&gt; The tidymodels framework is a **collection of packages** for modeling and machine learning using tidyverse principles. ~ [tidymodels.org](https://www.tidymodels.org/)


```r
install.packages("tidymodels")
library(tidymodels)
```
```
-- Attaching packages ----------------------------- tidymodels 0.1.1 --
v broom     0.7.2      v recipes   0.1.14
v dials     0.0.9      v rsample   0.0.8 
v dplyr     1.0.2      v tibble    3.0.4 
v ggplot2   3.3.2      v tidyr     1.1.2 
v infer     0.5.3      v tune      0.1.1 
v modeldata 0.1.0      v workflows 0.2.1 
v parsnip   0.1.4      v yardstick 0.0.7 
v purrr     0.3.4      

-- Conflicts ------------------------------- tidymodels_conflicts() --
x purrr::discard() masks scales::discard()
x dplyr::filter()  masks stats::filter()
x dplyr::lag()     masks stats::lag()
x recipes::step()  masks stats::step()
```

???
Explain:
- very similar when you load the whole tidyverse
- as you can see tidymodels loads also some of the tidyverse packages (however, usually you would load both at the beginning of your R session)
- again we have some conflicts here, so these functions override functions by the base `R` `stats` package
- `tidymodels v0.1.1`: relatively new package ecosystem, it is not unlikely that some of the functionalities or interfaces will change slightly in the future

---

## 2 Introduction to `tidymodels`

Remember, modeling is one of the main steps in our day-2-day data science workflow. And this is precisely where `tidymodels` fits in!
&lt;br&gt;&lt;br&gt;&lt;br&gt;
&lt;img src="https://www.tmwr.org/premade/data-science-model.svg" width="75%" height="75%" style="display: block; margin: auto;" /&gt;

.footnote[
*Source: [[1, ch. 1.3](#references)]*
]
???
src: https://d33wubrfki0l68.cloudfront.net/571b056757d68e6df81a3e3853f54d3c76ad6efc/32d37/diagrams/data-science.png

---

layout: false
class: middle, center, inverse

# 3 Himalayan Climbing Expeditions Data

---

## 3 Himalayan Climbing Expeditions Data

In order to illustrate the features of the `tidymodels` ecosystem, we use the [Himalayan Climbing Expeditions](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-09-22/readme.md) data set from the [`tidytuesday` project](https://github.com/rfordatascience/tidytuesday).


```r
# install.packages("tidytuesdayR")
tt_data &lt;- tidytuesdayR::tt_load(2020, week = 39)
```
```
&gt; --- Compiling #TidyTuesday Information for 2020-09-22 ----
&gt; --- There are 3 files available ---
&gt; --- Starting Download ---
&gt; 
&gt; 	Downloading file 1 of 3: `peaks.csv`
&gt; 	Downloading file 2 of 3: `members.csv`
&gt; 	Downloading file 3 of 3: `expeditions.csv`
&gt; 
&gt; --- Download complete ---
```

???
- Tidytuesday: social project to motivate the R online community to learn working with tools like ggplot2, dplyr and tidyr and applying them to real-world data
- around 50 different data sets right now

---

## 3 Himalayan Climbing Expeditions Data

The data set contains a large record of data spanning the 1905-2019 period about
- üèî the several **peaks** of the mountain range,
- üêæ **expeditions** during this period, and
- üßó‚Äç‚ôÄÔ∏è the **members** of each expedition.

--

In this workshop, we will try to predict the likelihood of an expedition coming to a lethal end. This, in turn, may then help us to derive drivers for a successful expedition and eventually reduce death rates.


```r
tt_data$members %&gt;% 
  skimr::skim() 
```

???
- use `skimr` package to get a high-level view of the data and most important descriptives

---

<style>.panelset{--panel-tab-active-foreground: #0051BA;--panel-tab-hover-foreground: #d22;}</style>

## 3 Himalayan Climbing Expeditions Data

.panelset[

.panel[
.panel-name[Output Pt. 1]
```
&gt; -- Data Summary ------------------------
&gt;                            Values    
&gt; Name                       Piped data
&gt; Number of rows             76519     
&gt; Number of columns          21        
&gt; _______________________              
&gt; Column type frequency:               
&gt;   character                10        
&gt;   logical                  6         
&gt;   numeric                  5         
&gt; ________________________             
&gt; Group variables            None      
```
]

.panel[
.panel-name[Output Pt. 2]
```
&gt; -- Variable type: character ---------------------------------------------------------------------------
&gt; # A tibble: 10 x 8
&gt;    skim_variable   n_missing complete_rate   min   max empty n_unique whitespace
&gt;  * &lt;chr&gt;               &lt;int&gt;         &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;      &lt;int&gt;
&gt;  1 expedition_id           0        1          9     9     0    10350          0
&gt;  2 member_id               0        1         12    12     0    76518          0
&gt;  3 peak_id                 0        1          4     4     0      391          0
&gt;  4 peak_name              15        1.00       4    25     0      390          0
&gt;  5 season                  0        1          6     7     0        5          0
&gt;  6 sex                     2        1.00       1     1     0        2          0
&gt;  7 citizenship            10        1.00       2    23     0      212          0
&gt;  8 expedition_role        21        1.00       4    25     0      524          0
&gt;  9 death_cause         75413        0.0145     3    27     0       12          0
&gt; 10 injury_type         74807        0.0224     3    27     0       11          0
```
]

.panel[
.panel-name[Output Pt. 3]
```
&gt; -- Variable type: logical -----------------------------------------------------------------------------
&gt; # A tibble: 6 x 5
&gt;   skim_variable n_missing complete_rate    mean count                 
&gt; * &lt;chr&gt;             &lt;int&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                 
&gt; 1 hired                 0             1 0.206   FAL: 60788, TRU: 15731
&gt; 2 success               0             1 0.382   FAL: 47320, TRU: 29199
&gt; 3 solo                  0             1 0.00158 FAL: 76398, TRU: 121  
&gt; 4 oxygen_used           0             1 0.238   FAL: 58286, TRU: 18233
&gt; 5 died                  0             1 0.0145  FAL: 75413, TRU: 1106 
&gt; 6 injured               0             1 0.0224  FAL: 74806, TRU: 1713 
```
]

.panel[
.panel-name[Output Pt. 4]
```
&gt; -- Variable type: numeric -----------------------------------------------------------------------------
&gt; # A tibble: 5 x 11
&gt;   skim_variable        n_missing complete_rate   mean     sd    p0   p25   p50   p75  p100 hist 
&gt; * &lt;chr&gt;                    &lt;int&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;
&gt; 1 year                         0        1      2000.    14.8  1905  1991  2004  2012  2019 ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñá
&gt; 2 age                       3497        0.954    37.3   10.4     7    29    36    44    85 ‚ñÅ‚ñá‚ñÖ‚ñÅ‚ñÅ
&gt; 3 highpoint_metres         21833        0.715  7471.  1040.   3800  6700  7400  8400  8850 ‚ñÅ‚ñÅ‚ñÜ‚ñÉ‚ñá
&gt; 4 death_height_metres      75451        0.0140 6593.  1308.    400  5800  6600  7550  8830 ‚ñÅ‚ñÅ‚ñÇ‚ñá‚ñÜ
&gt; 5 injury_height_metres     75510        0.0132 7050.  1214.    400  6200  7100  8000  8880 ‚ñÅ‚ñÅ‚ñÇ‚ñá‚ñá
```
]
]

???
**Pt. 1:**
- total of 76,519 expedition members
- categorization of data types

**Pt. 2:**
- three id columns, these are likely not supposed to end up in any predictive model -&gt; in any case, if you have an id variable with predictive value you should question in the data generating process behind the id column
- 391 different peaks, but only 390 different peak names
- with 76,519 climbers, almost 1000 died (75,413 non-death causes), and another 600 came back injured (74,807 non-injured) -&gt; imbalanced prediction task
- 524 different expedition roles
- why do we have five seasons? (probably an unknown category)

**Pt. 3:**
logical:
- never missing
- `hired` natives (around 20% of the expedition members)
- only 38% expeditions made it to the top (`success`)
- likely we can have expeditions that were successful, but where one or several member died
- died and injured corresponds to the numbers of `death_cause` and `injury_type`

**Pt. 4:**
numeric:
- hist of `year` expeditions took place more and more often in the two recent decades
- `age`: most climbers i would expect to be between 20-40, with few very old climbers (85), and some super young (7?!)
- `age` and `highpoint_metres` has a lot of missings!

usually, you would do a lot more EDA right now:
- plot of expedition year against success/failure rates -&gt; more recent expeditions likely more successful as you know more about the region/have better equipment
- plot of age against success/failure rates -&gt; younger, more athletic climbers more successful?
- check which peaks or seasons are most associated with climber deaths
- check if oxygen use is associated with death rates
- good practice is always to do a correlation matrix

---

## 3 Himalayan Climbing Expeditions Data


```r
climbers_df &lt;- tt_data$members %&gt;% 
  select(
    member_id, peak_name, season, year,sex, age, citizenship,
    expedition_role, hired, solo, oxygen_used, success, died) %&gt;% 
  filter(!is.na(sex), !is.na(citizenship), !is.na(peak_name), !is.na(expedition_role)) %&gt;% 
  mutate(across(where(~ is.character(.) || is.logical(.)), as.factor))

climbers_df
```

```
&gt; # A tibble: 76,471 x 13
&gt;    member_id peak_name season  year sex     age citizenship
&gt;    &lt;fct&gt;     &lt;fct&gt;     &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt;      
&gt;  1 AMAD7830~ Ama Dabl~ Autumn  1978 M        40 France     
&gt;  2 AMAD7830~ Ama Dabl~ Autumn  1978 M        41 France     
&gt;  3 AMAD7830~ Ama Dabl~ Autumn  1978 M        27 France     
&gt;  4 AMAD7830~ Ama Dabl~ Autumn  1978 M        40 France     
&gt;  5 AMAD7830~ Ama Dabl~ Autumn  1978 M        34 France     
&gt;  6 AMAD7830~ Ama Dabl~ Autumn  1978 M        25 France     
&gt;  7 AMAD7830~ Ama Dabl~ Autumn  1978 M        41 France     
&gt;  8 AMAD7830~ Ama Dabl~ Autumn  1978 M        29 France     
&gt;  9 AMAD7910~ Ama Dabl~ Spring  1979 M        35 USA        
&gt; 10 AMAD7910~ Ama Dabl~ Spring  1979 M        37 W Germany  
&gt; # ... with 76,461 more rows, and 6 more variables:
&gt; #   expedition_role &lt;fct&gt;, hired &lt;fct&gt;, solo &lt;fct&gt;,
&gt; #   oxygen_used &lt;fct&gt;, success &lt;fct&gt;, died &lt;fct&gt;
```


???
Note: After the removal of missing values in the `sex`, `citizenship`, `peak_name` and `expedition_role` predictor the data set shrinks 76,519 to 76,471 observations

---

```
&gt; -- Variable type: factor --------------------------------------------------------------------------------------
&gt; # A tibble: 2 x 6
&gt;   skim_variable   n_missing complete_rate ordered n_unique top_counts                                   
&gt; * &lt;chr&gt;               &lt;int&gt;         &lt;dbl&gt; &lt;lgl&gt;      &lt;int&gt; &lt;chr&gt;                                        
&gt; 1 sex                     0             1 FALSE          2 M: 69429, F: 7042                            
&gt; 2 expedition_role         0             1 FALSE        524 Cli: 44654, H-A: 14480, Lea: 10035, Exp: 1450
&gt; 
&gt; -- Variable type: numeric -------------------------------------------------------------------------------------
&gt; # A tibble: 2 x 11
&gt;   skim_variable n_missing complete_rate   mean    sd    p0   p25   p50   p75  p100 hist 
&gt; * &lt;chr&gt;             &lt;int&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;
&gt; 1 year                  0         1     2000.   14.8  1905  1991  2004  2012  2019 ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñá
&gt; 2 age                3486         0.954   37.3  10.4     7    29    36    44    85 ‚ñÅ‚ñá‚ñÖ‚ñÅ‚ñÅ
```


---



---

layout: false
class: center, middle

# 5-Minute Break&lt;br&gt;&lt;br&gt;‚òï üç©

---



---



---

layout: false
class: center, middle

# 5-Minute Break&lt;br&gt;&lt;br&gt;‚òï üç©

---



---



---



---

## 5 Additions to the `tidymodels` Ecosystem

Similar to the `tidyverse` ecosystem, there is already a promising supply of complementary packages that further improve the capabilities of `tidymodels`, e.g.:
.pull-left[

- `textrecipes`: Extra recipes for text processing&lt;br&gt;&lt;br&gt;
- `themis`: Extra recipes steps for dealing with imbalanced data&lt;br&gt;&lt;br&gt;
- `baguette`: Efficient model functions for bagging&lt;br&gt;&lt;br&gt;
- `stacks`: Tidy model stacking&lt;br&gt;&lt;br&gt;
- `probably`: Tools for post-processing class probability estimates&lt;br&gt;&lt;br&gt;
- `usemodels`: Boilerplate code for `tidymodels` analyses&lt;br&gt;&lt;br&gt;
]
.pull-right[
&lt;img src="https://tenor.com/view/shocked-po-kung-fu-panda-gif-4255877.gif" style="display: block; margin: auto;" /&gt;
]

???
- textrecipes as extension to the recipes package for natural language processing
- baguette as add-on to the parsnip package
- probably enables the identification of optimal probability thresholds and equivocal zones (uncertain probability regions)
- themis extends recipes package
- vip provides interpretability techniques, e.g., importance weights, PDP, SHAPley-values
- stacking: ensemble technique to integrate the predictions of multiple models into a meta-model

---

## 5 Additions to the `tidymodels` Ecosystem


```r
library(usemodels)
use_glmnet(died ~ ., data = climbers_df, verbose = F, prefix = "glmnet_mod")
```

.panelset[

.panel[
.panel-name[Recipe Template]
```
&gt; glmnet_mod_recipe &lt;- 
&gt;   recipe(formula = died ~ ., data = climbers_df) %&gt;% 
&gt;   step_novel(all_nominal(), -all_outcomes()) %&gt;% 
&gt;   step_dummy(all_nominal(), -all_outcomes()) %&gt;% 
&gt;   step_zv(all_predictors()) %&gt;% 
&gt;   step_normalize(all_predictors(), -all_nominal()) 
```
]
.panel[
.panel-name[Model Spec Template]
```
&gt; glmnet_mod_spec &lt;- 
&gt;   logistic_reg(penalty = tune(), mixture = tune()) %&gt;% 
&gt;   set_mode("classification") %&gt;% 
&gt;   set_engine("glmnet") 
```
]
.panel[
.panel-name[Workflow Template]
```
&gt; glmnet_mod_workflow &lt;- 
&gt;   workflow() %&gt;% 
&gt;   add_recipe(glmnet_mod_recipe) %&gt;% 
&gt;   add_model(glmnet_mod_spec) 
```
]
.panel[
.panel-name[Tuning Template]
```
&gt; glmnet_mod_grid &lt;- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20), 
&gt;     mixture = c(0.05, 0.2, 0.4, 0.6, 0.8, 1)) 
&gt; 
&gt; glmnet_mod_tune &lt;- 
&gt;   tune_grid(glmnet_mod_workflow, resamples = stop("add your rsample object"), grid = glmnet_mod_grid) 
```
]

]

--

**Supported model types:**

```r
ls("package:usemodels", pattern = "use_")
```

```
&gt; [1] "use_cubist"  "use_earth"   "use_glmnet"  "use_kknn"   
&gt; [5] "use_ranger"  "use_xgboost"
```

???
- `verbose = T` adds some additional comments for why some of the feature engineering steps are included
- recipe template contains minimal required steps
- proposed tuning grid might not be the best for your specific use case
- the template urges the user to manually specify the resampling method

---

## 5 Additions to the `tidymodels` Ecosystem

&lt;img src="./img/tidymodels-priorities.PNG" width="60%" height="60%" style="display: block; margin: auto;" /&gt;

.footnote[*Source: [RStudio blog](https://connect.rstudioservices.com/tidymodels-priorities-survey/README.html)*
]

---

## Thank You!

.pull-left[
.center[ü§î **Right now**]&lt;br&gt;&lt;br&gt;
&lt;img src="https://tenor.com/view/homer-daydreaming-thinking-simpsons-gif-8949118.gif" style="display: block; margin: auto;" /&gt;
]
.pull-right[
.center[ü§ì **After having mastered `tidymodels`**]&lt;br&gt;&lt;br&gt;
&lt;img src="https://tenor.com/view/homer-gif-10571731.gif" style="display: block; margin: auto;" /&gt;
]

---

## Thank You!

&lt;img src="./img/ai-meme.jpg" width="55%" height="55%" style="display: block; margin: auto;" /&gt;

.footnote[
*Source: [deeplearning.ai](https://www.linkedin.com/posts/deeplearningai_aifun-activity-6602264745171136512-QIwE)*
]

---

name: references

## References

[1]: **Kuhn, M./Silge, J. (2020):** Tidy Modeling in R. URL: https://www.tmwr.org (work-in-progress).

[2]: **Kuhn, M./Johnson, K. (2013):** Applied Predictive Modeling. Chapter 3 (Data Pre-processing). Springer: New York 2013.

[3]: **Mulner, C. (2020):** Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. URL: https://christophm.github.io/interpretable-ml-book/.

## Further Resources

Learn section of tidymodels.org. ULR: https://www.tidymodels.org/learn/.

TidyTuesday contributions by Julia Silge. URL: https://juliasilge.com/blog/.

## Credits

`tidymodels` [artworks and illustration](https://github.com/allisonhorst/stats-illustrations) are provided by Allison Horst.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
